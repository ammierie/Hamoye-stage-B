# -*- coding: utf-8 -*-
"""Hamoye stage B

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EiyNoBbhBOMSzaq_SAbyGWvDrzkbpud7
"""

import pandas as pd

energy = pd.read_csv('/content/energydata_complete (1).csv')
print(energy)

print(energy.columns)

#check for shape of dataframe
energy.shape

#Drop all duplicates
energy.drop_duplicates()

energy.dtypes

#Drop unnecessary columns
new_energy = energy.drop(['date'], axis = 1)

#Check null values
new_energy.isnull().sum()

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

#Firstly, we normalise our dataset to a common scale using the min max scaler
scaler = MinMaxScaler()
normalised_energy = pd.DataFrame(scaler.fit_transform(new_energy), columns = new_energy.columns)

#Separate dataset
x = normalised_energy.drop(['Appliances'], axis = 1)
y = normalised_energy['Appliances']

#Now, we split our dataset into the training and testing dataset.
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.3, random_state= 42)

#Predict T2 using T6
x1 = np.array(normalised_energy['T2'])
y1 = np.array(normalised_energy['T6'])

x1 = x1.reshape(-1,1)
x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size= 0.3, random_state= 42)

linear_model = LinearRegression()
linear_model.fit(x1_train, y1_train)
#obtain predictions
predicted_values = linear_model.predict(x1_test)

# R Squared
from sklearn.metrics import r2_score
r2_score = r2_score(y1_test, predicted_values)
round(r2_score, 2)

from sklearn.linear_model import LinearRegression
import numpy as np
lin_reg = LinearRegression()
lin_reg.fit(x_train, y_train)
predicted_y = lin_reg.predict(x_test)
train_score = lin_reg.score(x_train, y_train)
test_score = lin_reg.score(x_test, y_test)
print(train_score)
print(test_score)

#RSS
rss = np.sum(np.square(y_test - predicted_y))
round(rss, 2)

#MAE
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, predicted_y)
round(mae, 2)

#RME
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, predicted_y)
rmse = np.sqrt(mse)
round(rmse, 3)

#coefficient of determination
r_square = (y_test, predicted_y)
round(r2_score, 2)

#comparing the effects of regularisation
def weights(model, feat, col_name):
  weights = pd.Series(model.coef_, feat.columns).sort_values()
  weights_df = pd.DataFrame(weights).reset_index()
  weights_df.columns = [ 'Features' , col_name]
  weights_df[col_name].round( 3 )
  return weights_df
df = weights(lin_reg, x_train, 'Linear_model_weight')

print(df)

from sklearn.linear_model import Lasso
Lasso_reg = Lasso(alpha= 0.001 )
Lasso_reg.fit(x_train, y_train)
Lasso_pred = Lasso_reg.predict(x_test)
Lasso_tr_score = Lasso_reg.score(x_train, y_train)
Lasso_te_score = Lasso_reg.score(x_test, y_test)
print(Lasso_tr_score)
print(Lasso_te_score)

mse = mean_squared_error(y_test, lasso_pred)
rmse = np.sqrt(mse)
round(rmse, 3)